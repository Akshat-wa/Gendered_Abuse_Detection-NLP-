{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10992041,"sourceType":"datasetVersion","datasetId":6841842}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import classification_report\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nwarnings.filterwarnings(\"ignore\")\n\nMODEL_MAP = {\n    \"en\": \"distilroberta-base\",\n    \"hi\": \"bert-base-multilingual-cased\",\n    \"ta\": \"bert-base-multilingual-cased\"\n}\n\ndef preprocess_text(text, lang):\n    text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\\w+\", \"\", str(text))\n    return text.strip()\n\ndef load_multitask_data(lang, split=\"train\"):\n    label1_col = f\"{lang}_a1\"\n    label3_col = f\"{lang}_a3\"\n\n    # Select the correct base directory depending on the split.\n    if split.lower() == \"train\":\n        base_dir = \"/kaggle/input/uli-dataset/uli_dataset-main/training\"\n    elif split.lower() == \"test\":\n        base_dir = \"/kaggle/input/uli-dataset/uli_dataset-main/testing\"\n    else:\n        base_dir = \"\"\n\n    file_path = os.path.join(base_dir, f\"{split}_{lang}_l1.csv\")\n\n    if not os.path.exists(file_path):\n        print(f\"Missing file: {file_path}\")\n        return [], [], []\n\n    try:\n        df = pd.read_csv(file_path, engine=\"python\", on_bad_lines='skip')\n    except Exception as e:\n        print(f\"Error reading {file_path}: {e}\")\n        return [], [], []\n\n    if \"text\" not in df.columns or label1_col not in df.columns or label3_col not in df.columns:\n        print(f\"Missing required columns in {file_path}\")\n        return [], [], []\n\n    texts, label1s, label3s = [], [], []\n    for _, row in df.iterrows():\n        try:\n            text = preprocess_text(row[\"text\"], lang)\n            l1 = int(float(str(row[label1_col]).replace('.0', '')))\n            l3 = int(float(str(row[label3_col]).replace('.0', '')))\n            texts.append(text)\n            label1s.append(l1)\n            label3s.append(l3)\n        except Exception as e:\n            continue\n    return texts, label1s, label3s\n\n\nclass MultiTaskDataset(Dataset):\n    def __init__(self, encodings, labels1, labels3):\n        self.encodings = encodings\n        self.labels1 = labels1\n        self.labels3 = labels3\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels1': torch.tensor(self.labels1[idx], dtype=torch.long),\n            'labels3': torch.tensor(self.labels3[idx], dtype=torch.long)\n        }\n\n    def __len__(self):\n        return len(self.labels1)\n\nclass MultiTaskBiLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=300, hidden_size=128, dropout_prob=0.2, num_classes=2):\n        \n        super(MultiTaskBiLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=1,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.dropout = nn.Dropout(dropout_prob)\n        \n        self.classifier1 = nn.Linear(2 * hidden_size, num_classes)\n        self.classifier3 = nn.Linear(2 * hidden_size, num_classes) \n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask=None, labels1=None, labels3=None):\n        embedded = self.embedding(input_ids) \n        lstm_out, _ = self.lstm(embedded)  \n        if attention_mask is not None:\n            mask = attention_mask.unsqueeze(-1)  # (batch_size, seq_len, 1)\n            lstm_out = lstm_out * mask  # Zero-out pads\n            lengths = mask.sum(1) \n            rep = lstm_out.sum(1) / lengths.clamp(min=1e-9)\n        else:\n            rep = lstm_out.mean(dim=1)\n        \n        rep = self.dropout(rep)\n        logits1 = self.classifier1(rep)\n        logits3 = self.classifier3(rep)\n        \n        loss = None\n        if labels1 is not None and labels3 is not None:\n            loss1 = self.loss_fct(logits1, labels1)\n            loss3 = self.loss_fct(logits3, labels3)\n            loss = (loss1 + loss3) / 2.0\n\n        return {\n            'loss': loss,\n            'logits1': logits1,\n            'logits3': logits3\n        }\n\ndef compute_multitask_metrics(eval_pred):\n    if isinstance(eval_pred.predictions, tuple):\n        logits1, logits3 = eval_pred.predictions\n    elif isinstance(eval_pred.predictions, dict):\n        logits1 = eval_pred.predictions.get('logits1')\n        logits3 = eval_pred.predictions.get('logits3')\n    else:\n        raise TypeError(\"Unexpected type for predictions.\")\n\n    if isinstance(eval_pred.label_ids, dict):\n        labels1 = eval_pred.label_ids.get('labels1')\n        labels3 = eval_pred.label_ids.get('labels3')\n    elif isinstance(eval_pred.label_ids, (list, tuple)) and len(eval_pred.label_ids) == 2:\n        labels1, labels3 = eval_pred.label_ids\n    else:\n        raise TypeError(\"Unexpected type for label_ids.\")\n\n    pred1 = np.argmax(logits1, axis=1)\n    pred3 = np.argmax(logits3, axis=1)\n    report1 = classification_report(labels1, pred1, output_dict=True, zero_division=0)\n    report3 = classification_report(labels3, pred3, output_dict=True, zero_division=0)\n\n    return {\n        \"f1_task1\": report1[\"weighted avg\"][\"f1-score\"],\n        \"acc_task1\": report1[\"accuracy\"],\n        \"f1_task3\": report3[\"weighted avg\"][\"f1-score\"],\n        \"acc_task3\": report3[\"accuracy\"]\n    }\n\ndef train_multitask(lang):\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_MAP[lang])\n    train_texts, train_labels1, train_labels3 = load_multitask_data(lang, \"train\")\n    test_texts, test_labels1, test_labels3 = load_multitask_data(lang, \"test\")\n    train_enc = tokenizer(train_texts, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n    test_enc = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n\n    train_dataset = MultiTaskDataset(train_enc, train_labels1, train_labels3)\n    test_dataset = MultiTaskDataset(test_enc, test_labels1, test_labels3)\n    vocab_size = tokenizer.vocab_size\n    model = MultiTaskBiLSTM(vocab_size=vocab_size, embedding_dim=300, hidden_size=128)\n    model.config = type(\"DummyConfig\", (), {})()\n\n    args = TrainingArguments(\n        output_dir=f\"./multitask_bilstm_{lang}\",\n        per_device_train_batch_size=32,\n        per_device_eval_batch_size=64,\n        num_train_epochs=3,\n        learning_rate=3e-5,\n        logging_steps=100,\n        eval_strategy=\"epoch\",\n        save_strategy=\"no\",\n        report_to=\"none\",\n        disable_tqdm=True\n    )\n\n    def custom_compute_metrics(eval_pred):\n        return compute_multitask_metrics(eval_pred)\n\n    def collate_fn(batch):\n        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n        attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n        labels1 = torch.stack([item[\"labels1\"] for item in batch])\n        labels3 = torch.stack([item[\"labels3\"] for item in batch])\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels1\": labels1,\n            \"labels3\": labels3\n        }\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        data_collator=collate_fn,\n        compute_metrics=custom_compute_metrics\n    )\n\n    trainer.train()\n    metrics = trainer.evaluate()\n    print(f\"\\n{lang.upper()} Results:\")\n    print(f\"Gendered Abuse Task - F1: {metrics.get('eval_f1_task1', 0):.3f}, Accuracy: {metrics.get('eval_acc_task1', 0):.3f}\")\n    print(f\"Explicit Language Task - F1: {metrics.get('eval_f1_task3', 0):.3f}, Accuracy: {metrics.get('eval_acc_task3', 0):.3f}\")\n    print(\"⎯\" * 30)\n\nif __name__ == \"__main__\":\n    for lang in [\"en\", \"hi\", \"ta\"]:\n        train_multitask(lang)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T11:30:00.211161Z","iopub.execute_input":"2025-04-15T11:30:00.211449Z","iopub.status.idle":"2025-04-15T11:30:07.744539Z","shell.execute_reply.started":"2025-04-15T11:30:00.211431Z","shell.execute_reply":"2025-04-15T11:30:07.743809Z"}},"outputs":[{"name":"stdout","text":"{'eval_loss': 0.6825830340385437, 'eval_f1_task1': 0.6348073591287848, 'eval_acc_task1': 0.7065217391304348, 'eval_f1_task3': 0.4775268210050818, 'eval_acc_task3': 0.5869565217391305, 'eval_runtime': 0.0289, 'eval_samples_per_second': 3187.24, 'eval_steps_per_second': 34.644, 'epoch': 1.0}\n{'eval_loss': 0.6822375655174255, 'eval_f1_task1': 0.6348073591287848, 'eval_acc_task1': 0.7065217391304348, 'eval_f1_task3': 0.4775268210050818, 'eval_acc_task3': 0.5869565217391305, 'eval_runtime': 0.0245, 'eval_samples_per_second': 3758.483, 'eval_steps_per_second': 40.853, 'epoch': 2.0}\n{'eval_loss': 0.6820657849311829, 'eval_f1_task1': 0.6415396198004893, 'eval_acc_task1': 0.717391304347826, 'eval_f1_task3': 0.4775268210050818, 'eval_acc_task3': 0.5869565217391305, 'eval_runtime': 0.0236, 'eval_samples_per_second': 3905.785, 'eval_steps_per_second': 42.454, 'epoch': 3.0}\n{'train_runtime': 0.1431, 'train_samples_per_second': 503.063, 'train_steps_per_second': 20.961, 'train_loss': 0.6854918797810873, 'epoch': 3.0}\n{'eval_loss': 0.6820657849311829, 'eval_f1_task1': 0.6415396198004893, 'eval_acc_task1': 0.717391304347826, 'eval_f1_task3': 0.4775268210050818, 'eval_acc_task3': 0.5869565217391305, 'eval_runtime': 0.024, 'eval_samples_per_second': 3839.18, 'eval_steps_per_second': 41.73, 'epoch': 3.0}\n\nEN Results:\nGendered Abuse Task - F1: 0.642, Accuracy: 0.717\nExplicit Language Task - F1: 0.478, Accuracy: 0.587\n⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9f551c2fb5545dabc4a0cb3738625f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a2cd9343b074addb9da34378fb9dfc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a1a03f21da4a4083bec5d65d440918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f44b15da7ea4faab94b5c609f7766eb"}},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.6881389617919922, 'eval_f1_task1': 0.6047152771175968, 'eval_acc_task1': 0.5668789808917197, 'eval_f1_task3': 0.5227893125982297, 'eval_acc_task3': 0.5796178343949044, 'eval_runtime': 0.1008, 'eval_samples_per_second': 4673.855, 'eval_steps_per_second': 39.693, 'epoch': 1.0}\n{'eval_loss': 0.6874972581863403, 'eval_f1_task1': 0.6180360314495809, 'eval_acc_task1': 0.583864118895966, 'eval_f1_task3': 0.5208272088845338, 'eval_acc_task3': 0.5796178343949044, 'eval_runtime': 0.0963, 'eval_samples_per_second': 4888.793, 'eval_steps_per_second': 41.518, 'epoch': 2.0}\n{'eval_loss': 0.6871768832206726, 'eval_f1_task1': 0.6276601483361461, 'eval_acc_task1': 0.5966029723991507, 'eval_f1_task3': 0.5188205855590644, 'eval_acc_task3': 0.5796178343949044, 'eval_runtime': 0.093, 'eval_samples_per_second': 5064.273, 'eval_steps_per_second': 43.009, 'epoch': 3.0}\n{'train_runtime': 0.3965, 'train_samples_per_second': 189.142, 'train_steps_per_second': 7.566, 'train_loss': 0.6920965512593588, 'epoch': 3.0}\n{'eval_loss': 0.6871768832206726, 'eval_f1_task1': 0.6276601483361461, 'eval_acc_task1': 0.5966029723991507, 'eval_f1_task3': 0.5188205855590644, 'eval_acc_task3': 0.5796178343949044, 'eval_runtime': 0.0833, 'eval_samples_per_second': 5655.031, 'eval_steps_per_second': 48.026, 'epoch': 3.0}\n\nHI Results:\nGendered Abuse Task - F1: 0.628, Accuracy: 0.597\nExplicit Language Task - F1: 0.519, Accuracy: 0.580\n⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯\n{'eval_loss': 0.6934882402420044, 'eval_f1_task1': 0.46458613438135615, 'eval_acc_task1': 0.5017064846416383, 'eval_f1_task3': 0.4259804399227825, 'eval_acc_task3': 0.4709897610921502, 'eval_runtime': 0.0738, 'eval_samples_per_second': 3971.288, 'eval_steps_per_second': 40.662, 'epoch': 1.0}\n{'eval_loss': 0.6928065419197083, 'eval_f1_task1': 0.46663289507392014, 'eval_acc_task1': 0.5017064846416383, 'eval_f1_task3': 0.48091848002814014, 'eval_acc_task3': 0.5051194539249146, 'eval_runtime': 0.0696, 'eval_samples_per_second': 4208.524, 'eval_steps_per_second': 43.091, 'epoch': 2.0}\n{'eval_loss': 0.6925430297851562, 'eval_f1_task1': 0.47589608987057214, 'eval_acc_task1': 0.5085324232081911, 'eval_f1_task3': 0.4913733652285626, 'eval_acc_task3': 0.5119453924914675, 'eval_runtime': 0.0805, 'eval_samples_per_second': 3638.754, 'eval_steps_per_second': 37.257, 'epoch': 3.0}\n{'train_runtime': 0.6056, 'train_samples_per_second': 757.907, 'train_steps_per_second': 14.861, 'train_loss': 0.6937576399909126, 'epoch': 3.0}\n{'eval_loss': 0.6925430297851562, 'eval_f1_task1': 0.47589608987057214, 'eval_acc_task1': 0.5085324232081911, 'eval_f1_task3': 0.4913733652285626, 'eval_acc_task3': 0.5119453924914675, 'eval_runtime': 0.0688, 'eval_samples_per_second': 4257.823, 'eval_steps_per_second': 43.595, 'epoch': 3.0}\n\nTA Results:\nGendered Abuse Task - F1: 0.476, Accuracy: 0.509\nExplicit Language Task - F1: 0.491, Accuracy: 0.512\n⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport re\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import classification_report\nfrom transformers import (AutoTokenizer, Trainer, TrainingArguments, \n                          XLMRobertaModel)\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nwarnings.filterwarnings(\"ignore\")\n\ndef preprocess_text(text, lang):\n    \"\"\"Removing URLs, mentions, hashtags and extra whitespaces.\"\"\"\n    text = re.sub(r\"http\\S+|www\\S+|@\\w+|#\\w+\", \"\", str(text))\n    return text.strip()\n\ndef load_multitask_data(lang, split=\"train\"):\n    label1_col = f\"{lang}_a1\"\n    label3_col = f\"{lang}_a3\"\n\n    if split.lower() == \"train\":\n        base_dir = \"/kaggle/input/uli-dataset/uli_dataset-main/training\"\n    elif split.lower() == \"test\":\n        base_dir = \"/kaggle/input/uli-dataset/uli_dataset-main/testing\"\n    else:\n        base_dir = \"\"\n\n    file_path = os.path.join(base_dir, f\"{split}_{lang}_l1.csv\")\n\n    if not os.path.exists(file_path):\n        print(f\"Missing file: {file_path}\")\n        return [], [], []\n\n    try:\n        df = pd.read_csv(file_path, engine=\"python\", on_bad_lines='skip')\n    except Exception as e:\n        print(f\"Error reading {file_path}: {e}\")\n        return [], [], []\n\n    if \"text\" not in df.columns or label1_col not in df.columns or label3_col not in df.columns:\n        print(f\"Missing required columns in {file_path}\")\n        return [], [], []\n\n    texts, label1s, label3s = [], [], []\n    for _, row in df.iterrows():\n        try:\n            text = preprocess_text(row[\"text\"], lang)\n            l1 = int(float(str(row[label1_col]).replace('.0', '')))\n            l3 = int(float(str(row[label3_col]).replace('.0', '')))\n            texts.append(text)\n            label1s.append(l1)\n            label3s.append(l3)\n        except Exception as e:\n            continue\n    return texts, label1s, label3s\n\nclass MultiTaskDataset(Dataset):\n    def __init__(self, encodings, labels1, labels3):\n        self.encodings = encodings\n        self.labels1 = labels1\n        self.labels3 = labels3\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels1': torch.tensor(self.labels1[idx], dtype=torch.long),\n            'labels3': torch.tensor(self.labels3[idx], dtype=torch.long)\n        }\n\n    def __len__(self):\n        return len(self.labels1)\n\nclass MultiTaskXRoBERTa(nn.Module):\n    def __init__(self, num_classes=2, dropout_prob=0.2):\n        super(MultiTaskXRoBERTa, self).__init__()\n        self.xroberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")\n        hidden_size = self.xroberta.config.hidden_size\n        \n        self.dropout = nn.Dropout(dropout_prob)\n        self.classifier1 = nn.Linear(hidden_size, num_classes)\n        self.classifier3 = nn.Linear(hidden_size, num_classes)\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels1=None, labels3=None):\n        outputs = self.xroberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state.mean(dim=1)\n        pooled_output = self.dropout(pooled_output)\n        logits1 = self.classifier1(pooled_output)\n        logits3 = self.classifier3(pooled_output)\n        \n        loss = None\n        if labels1 is not None and labels3 is not None:\n            loss1 = self.loss_fct(logits1, labels1)\n            loss3 = self.loss_fct(logits3, labels3)\n            loss = (loss1 + loss3) / 2.0\n        \n        return {\n            'loss': loss,\n            'logits1': logits1,\n            'logits3': logits3\n        }\n\ndef compute_multitask_metrics(eval_pred):\n    if isinstance(eval_pred.predictions, tuple):\n        logits1, logits3 = eval_pred.predictions\n    elif isinstance(eval_pred.predictions, dict):\n        logits1 = eval_pred.predictions.get('logits1')\n        logits3 = eval_pred.predictions.get('logits3')\n    else:\n        raise TypeError(\"Unexpected type for predictions.\")\n\n    if isinstance(eval_pred.label_ids, dict):\n        labels1 = eval_pred.label_ids.get('labels1')\n        labels3 = eval_pred.label_ids.get('labels3')\n    elif isinstance(eval_pred.label_ids, (list, tuple)) and len(eval_pred.label_ids) == 2:\n        labels1, labels3 = eval_pred.label_ids\n    else:\n        raise TypeError(\"Unexpected type for label_ids.\")\n\n    pred1 = np.argmax(logits1, axis=1)\n    pred3 = np.argmax(logits3, axis=1)\n    \n    report1 = classification_report(labels1, pred1, output_dict=True, zero_division=0)\n    report3 = classification_report(labels3, pred3, output_dict=True, zero_division=0)\n\n    return {\n        \"precision_task1\": report1[\"weighted avg\"][\"precision\"],\n        \"recall_task1\": report1[\"weighted avg\"][\"recall\"],\n        \"f1_task1\": report1[\"weighted avg\"][\"f1-score\"],\n        \"acc_task1\": report1[\"accuracy\"],\n        \"precision_task3\": report3[\"weighted avg\"][\"precision\"],\n        \"recall_task3\": report3[\"weighted avg\"][\"recall\"],\n        \"f1_task3\": report3[\"weighted avg\"][\"f1-score\"],\n        \"acc_task3\": report3[\"accuracy\"]\n    }\n\ndef train_multitask(lang):\n    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n    \n    train_texts, train_labels1, train_labels3 = load_multitask_data(lang, \"train\")\n    test_texts, test_labels1, test_labels3 = load_multitask_data(lang, \"test\")\n\n    train_enc = tokenizer(train_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    test_enc = tokenizer(test_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n\n    train_dataset = MultiTaskDataset(train_enc, train_labels1, train_labels3)\n    test_dataset = MultiTaskDataset(test_enc, test_labels1, test_labels3)\n\n    model = MultiTaskXRoBERTa(num_classes=2, dropout_prob=0.2)\n    model.config = type(\"DummyConfig\", (), {})() \n\n    args = TrainingArguments(\n        output_dir=f\"./multitask_xroberta_{lang}\",\n        per_device_train_batch_size=8,  \n        per_device_eval_batch_size=16,\n        num_train_epochs=3,\n        learning_rate=3e-5,\n        logging_steps=100,\n        eval_strategy=\"epoch\",\n        save_strategy=\"no\",\n        report_to=\"none\",\n        save_total_limit=1,\n        disable_tqdm=False  # tqdm progress bars\n    )\n\n    def custom_compute_metrics(eval_pred):\n        return compute_multitask_metrics(eval_pred)\n\n    def collate_fn(batch):\n        input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n        attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n        labels1 = torch.stack([item[\"labels1\"] for item in batch])\n        labels3 = torch.stack([item[\"labels3\"] for item in batch])\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels1\": labels1,\n            \"labels3\": labels3\n        }\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        data_collator=collate_fn,\n        compute_metrics=custom_compute_metrics\n    )\n\n    trainer.train()\n    metrics = trainer.evaluate()\n\n    save_path = f\"best_model_{lang}.pth\"\n    torch.save(model.state_dict(), save_path)\n    print(f\"Saved best model for {lang.upper()} to {save_path}\")\n\n    print(f\"\\n{'='*40}\")\n    print(f\"Results for language: {lang.upper()}\")\n    print(f\"{'-'*40}\")\n    print(\"Gendered Abuse Task:\")\n    print(f\"  Precision: {metrics.get('eval_precision_task1', 0):.3f}\")\n    print(f\"  Recall:    {metrics.get('eval_recall_task1', 0):.3f}\")\n    print(f\"  F1 Score:  {metrics.get('eval_f1_task1', 0):.3f}\")\n    print(f\"  Accuracy:  {metrics.get('eval_acc_task1', 0):.3f}\")\n    print(f\"{'-'*40}\")\n    print(\"Explicit Language Task:\")\n    print(f\"  Precision: {metrics.get('eval_precision_task3', 0):.3f}\")\n    print(f\"  Recall:    {metrics.get('eval_recall_task3', 0):.3f}\")\n    print(f\"  F1 Score:  {metrics.get('eval_f1_task3', 0):.3f}\")\n    print(f\"  Accuracy:  {metrics.get('eval_acc_task3', 0):.3f}\")\n    print(f\"{'='*40}\\n\")\n\nif __name__ == \"__main__\":\n    for lang in [\"en\", \"hi\", \"ta\"]:\n        train_multitask(lang)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:32:02.171496Z","iopub.execute_input":"2025-04-15T15:32:02.171783Z","iopub.status.idle":"2025-04-15T15:35:14.205074Z","shell.execute_reply.started":"2025-04-15T15:32:02.171760Z","shell.execute_reply":"2025-04-15T15:35:14.204219Z"}},"outputs":[{"name":"stderr","text":"2025-04-15 15:32:26.574823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744731147.023458      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744731147.150472      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80189025ae5d43b593b2db6ef545d85a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c796f426664d45bb2b7e150a037620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88b43e172f0642c9a391eb2184185028"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"081a7e9fdf2c4928815794632fe578ba"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691b16ea97e14b6ebdc3b605822935af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:09, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision Task1</th>\n      <th>Recall Task1</th>\n      <th>F1 Task1</th>\n      <th>Acc Task1</th>\n      <th>Precision Task3</th>\n      <th>Recall Task3</th>\n      <th>F1 Task3</th>\n      <th>Acc Task3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.638697</td>\n      <td>0.530364</td>\n      <td>0.728261</td>\n      <td>0.613754</td>\n      <td>0.728261</td>\n      <td>0.331876</td>\n      <td>0.576087</td>\n      <td>0.421139</td>\n      <td>0.576087</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.647282</td>\n      <td>0.530364</td>\n      <td>0.728261</td>\n      <td>0.613754</td>\n      <td>0.728261</td>\n      <td>0.331876</td>\n      <td>0.576087</td>\n      <td>0.421139</td>\n      <td>0.576087</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.650696</td>\n      <td>0.530364</td>\n      <td>0.728261</td>\n      <td>0.613754</td>\n      <td>0.728261</td>\n      <td>0.331876</td>\n      <td>0.576087</td>\n      <td>0.421139</td>\n      <td>0.576087</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Saved best model for EN to best_model_en.pth\n\n========================================\nResults for language: EN\n----------------------------------------\nGendered Abuse Task:\n  Precision: 0.530\n  Recall:    0.728\n  F1 Score:  0.614\n  Accuracy:  0.728\n----------------------------------------\nExplicit Language Task:\n  Precision: 0.332\n  Recall:    0.576\n  F1 Score:  0.421\n  Accuracy:  0.576\n========================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision Task1</th>\n      <th>Recall Task1</th>\n      <th>F1 Task1</th>\n      <th>Acc Task1</th>\n      <th>Precision Task3</th>\n      <th>Recall Task3</th>\n      <th>F1 Task3</th>\n      <th>Acc Task3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.588825</td>\n      <td>0.633900</td>\n      <td>0.796178</td>\n      <td>0.705832</td>\n      <td>0.796178</td>\n      <td>0.405696</td>\n      <td>0.636943</td>\n      <td>0.495675</td>\n      <td>0.636943</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.573084</td>\n      <td>0.633900</td>\n      <td>0.796178</td>\n      <td>0.705832</td>\n      <td>0.796178</td>\n      <td>0.405696</td>\n      <td>0.636943</td>\n      <td>0.495675</td>\n      <td>0.636943</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.575952</td>\n      <td>0.633900</td>\n      <td>0.796178</td>\n      <td>0.705832</td>\n      <td>0.796178</td>\n      <td>0.405696</td>\n      <td>0.636943</td>\n      <td>0.495675</td>\n      <td>0.636943</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [15/15 00:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Saved best model for HI to best_model_hi.pth\n\n========================================\nResults for language: HI\n----------------------------------------\nGendered Abuse Task:\n  Precision: 0.634\n  Recall:    0.796\n  F1 Score:  0.706\n  Accuracy:  0.796\n----------------------------------------\nExplicit Language Task:\n  Precision: 0.406\n  Recall:    0.637\n  F1 Score:  0.496\n  Accuracy:  0.637\n========================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 00:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision Task1</th>\n      <th>Recall Task1</th>\n      <th>F1 Task1</th>\n      <th>Acc Task1</th>\n      <th>Precision Task3</th>\n      <th>Recall Task3</th>\n      <th>F1 Task3</th>\n      <th>Acc Task3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.661805</td>\n      <td>0.719207</td>\n      <td>0.672355</td>\n      <td>0.625698</td>\n      <td>0.672355</td>\n      <td>0.369067</td>\n      <td>0.607509</td>\n      <td>0.459178</td>\n      <td>0.607509</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.603759</td>\n      <td>0.739990</td>\n      <td>0.730375</td>\n      <td>0.717313</td>\n      <td>0.730375</td>\n      <td>0.649248</td>\n      <td>0.624573</td>\n      <td>0.516794</td>\n      <td>0.624573</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.552215</td>\n      <td>0.728423</td>\n      <td>0.730375</td>\n      <td>0.727425</td>\n      <td>0.730375</td>\n      <td>0.844293</td>\n      <td>0.839590</td>\n      <td>0.835072</td>\n      <td>0.839590</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Saved best model for TA to best_model_ta.pth\n\n========================================\nResults for language: TA\n----------------------------------------\nGendered Abuse Task:\n  Precision: 0.728\n  Recall:    0.730\n  F1 Score:  0.727\n  Accuracy:  0.730\n----------------------------------------\nExplicit Language Task:\n  Precision: 0.844\n  Recall:    0.840\n  F1 Score:  0.835\n  Accuracy:  0.840\n========================================\n\n","output_type":"stream"}],"execution_count":1}]}